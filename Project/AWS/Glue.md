데이터를 읽고, 분류하여, 저장하는 서비스로 순서는 다음과 같다.

1. 데이터를 crawler를 통해 읽어온다.
2. 테이블 및 파티션을 생성한다.
3. 위 메타데이터를 저장한다.

위 방식을 통해 만들어진 메타데이터를 Amazon에서 지원하는 RDS, DynamoDB, S3에 저장한다.1

1. IAM 권한이 없는 사용자를 생성한다.
2. S3, RDS, DynamoDB 중 하나에 사용할 데이터를 업로드 한다.
3. glue에서 사용자에 권한을 부여해준다. (사용자에 해당 storage read-only 권한 부여) : 저장소의 데이터를 읽고, 해당 데이터의 스키마를 따서 테이블로 만들어줄 것이기에 쓸 권한은 필요 없다.
4. crawler 세팅
5. crawler run 후에 테이블 확인 : 테이블이 정상적으로 생성 되었으면 성공
6. ETL job을 통해 데이터셋 정규화
# Job

- ETL 파이프라인이 자동으로 실행되는 단위
#### ETL
- Extract : 데이터 추출
- Tranfer : 데이터 변환
- Load : 데이터 적재

# Data Catalog

- AWS에서 데이터베이스 내에 여러 데이터베이스와 유사한 catalog가 존재하도록 설계(스키마와 유사)
- 각 데이터베이스 catalog는 여러 테이블로 구성
- 따라서 

#### metadata
- 소스타입, 데이터포멧, 컬럼정보를 담고있는 데이터
- 데이터를 설명하기 위한 데이터로 실제 데이터들을 나타내거나, 알려주는 데이터를 의미한다.

# Table 생성방법

1. crawlers
2. 수동생성(콘솔,API를 통해 직접 생성)
3. AWS SDK 
4. Hive Metastore 마이그레이션
5. CloudFormation 등 laC 도구 사용

1번이 가장 많이 사용됨.

# Crawlers

#### Classifier
- custom classifier : 사용자가 직접 식별자를 생성하여 데이터를 가져오는 방식
- built-in classifier : 내장된 식별자로 데이터를 가져오는 방식

만약 두 classifier가 동시에 데이터를 크롤링 해올때, 먼저 온 결과물을 채택

크롤러의 역할은 데이터를 읽고, 메타데이터를 생성해서, 쓰기를 해야 한다. 위 동작을 하기 위해서는 크롤러에 권한을 부여해야 하는데, 이는 IAM Role을 통해 부여하게 된다.